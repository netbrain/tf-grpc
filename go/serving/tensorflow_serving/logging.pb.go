// Code generated by protoc-gen-go. DO NOT EDIT.
// source: tensorflow_serving/core/logging.proto

package tensorflow_serving // import "github.com/netbrain/tf-grpc/go/serving/tensorflow_serving"

import proto "github.com/golang/protobuf/proto"
import fmt "fmt"
import math "math"

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package

// Metadata logged along with the request logs.
type LogMetadata struct {
	ModelSpec            *ModelSpec      `protobuf:"bytes,1,opt,name=model_spec,json=modelSpec,proto3" json:"model_spec,omitempty"`
	SamplingConfig       *SamplingConfig `protobuf:"bytes,2,opt,name=sampling_config,json=samplingConfig,proto3" json:"sampling_config,omitempty"`
	XXX_NoUnkeyedLiteral struct{}        `json:"-"`
	XXX_unrecognized     []byte          `json:"-"`
	XXX_sizecache        int32           `json:"-"`
}

func (m *LogMetadata) Reset()         { *m = LogMetadata{} }
func (m *LogMetadata) String() string { return proto.CompactTextString(m) }
func (*LogMetadata) ProtoMessage()    {}
func (*LogMetadata) Descriptor() ([]byte, []int) {
	return fileDescriptor_logging_270aa0cd5cfe8fc3, []int{0}
}
func (m *LogMetadata) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_LogMetadata.Unmarshal(m, b)
}
func (m *LogMetadata) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_LogMetadata.Marshal(b, m, deterministic)
}
func (dst *LogMetadata) XXX_Merge(src proto.Message) {
	xxx_messageInfo_LogMetadata.Merge(dst, src)
}
func (m *LogMetadata) XXX_Size() int {
	return xxx_messageInfo_LogMetadata.Size(m)
}
func (m *LogMetadata) XXX_DiscardUnknown() {
	xxx_messageInfo_LogMetadata.DiscardUnknown(m)
}

var xxx_messageInfo_LogMetadata proto.InternalMessageInfo

func (m *LogMetadata) GetModelSpec() *ModelSpec {
	if m != nil {
		return m.ModelSpec
	}
	return nil
}

func (m *LogMetadata) GetSamplingConfig() *SamplingConfig {
	if m != nil {
		return m.SamplingConfig
	}
	return nil
}

func init() {
	proto.RegisterType((*LogMetadata)(nil), "tensorflow.serving.LogMetadata")
}

func init() {
	proto.RegisterFile("tensorflow_serving/core/logging.proto", fileDescriptor_logging_270aa0cd5cfe8fc3)
}

var fileDescriptor_logging_270aa0cd5cfe8fc3 = []byte{
	// 228 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x6c, 0x8f, 0x31, 0x4b, 0x04, 0x31,
	0x10, 0x85, 0x89, 0x85, 0x60, 0x0e, 0x14, 0x52, 0x1d, 0x07, 0x82, 0x9c, 0x08, 0x36, 0x26, 0xa0,
	0x95, 0x20, 0x16, 0x5a, 0xea, 0x35, 0x77, 0x9d, 0xcd, 0x92, 0xcd, 0xcd, 0x8e, 0x81, 0x24, 0x13,
	0x92, 0xa8, 0x7f, 0xc5, 0x9f, 0x6a, 0x29, 0x66, 0x6f, 0x59, 0x64, 0xb7, 0xcb, 0xc0, 0xf7, 0xbe,
	0x97, 0xc7, 0xaf, 0x0a, 0x84, 0x4c, 0xa9, 0x73, 0xf4, 0xd5, 0x64, 0x48, 0x9f, 0x36, 0xa0, 0x32,
	0x94, 0x40, 0x39, 0x42, 0xb4, 0x01, 0x65, 0x4c, 0x54, 0x48, 0x88, 0x11, 0x93, 0x07, 0x6c, 0x75,
	0x39, 0x13, 0xd5, 0xd1, 0x66, 0xe5, 0x69, 0x0f, 0xae, 0x0f, 0xae, 0xe4, 0xac, 0x3f, 0x74, 0x16,
	0x87, 0x86, 0xa6, 0x3f, 0x7b, 0x7e, 0xfd, 0xcd, 0xf8, 0xe2, 0x95, 0x70, 0x03, 0x45, 0xef, 0x75,
	0xd1, 0xe2, 0x81, 0xf3, 0xaa, 0x6b, 0x72, 0x04, 0xb3, 0x64, 0x17, 0xec, 0x7a, 0x71, 0x7b, 0x2e,
	0xa7, 0xbf, 0x91, 0x9b, 0x3f, 0x6a, 0x17, 0xc1, 0x6c, 0x4f, 0xfc, 0xf0, 0x14, 0x2f, 0xfc, 0x2c,
	0x6b, 0x1f, 0xdd, 0x58, 0xb3, 0x3c, 0xaa, 0x8a, 0xf5, 0x9c, 0x62, 0x77, 0x40, 0x9f, 0x2b, 0xb9,
	0x3d, 0xcd, 0xff, 0xee, 0xa7, 0xc7, 0xb7, 0x7b, 0xb4, 0xe5, 0xfd, 0xa3, 0x95, 0x86, 0xbc, 0x0a,
	0x50, 0xda, 0xa4, 0x6d, 0x50, 0xa5, 0xbb, 0xc1, 0x14, 0x8d, 0x42, 0x52, 0xc3, 0xc0, 0xe9, 0xe6,
	0x1f, 0xc6, 0xda, 0xe3, 0xba, 0xf0, 0xee, 0x37, 0x00, 0x00, 0xff, 0xff, 0x1a, 0x42, 0x86, 0xdc,
	0x73, 0x01, 0x00, 0x00,
}
