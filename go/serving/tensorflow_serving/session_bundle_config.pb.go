// Code generated by protoc-gen-go. DO NOT EDIT.
// source: tensorflow_serving/servables/tensorflow/session_bundle_config.proto

package tensorflow_serving // import "github.com/netbrain/tf-grpc/go/serving/tensorflow_serving"

import proto "github.com/golang/protobuf/proto"
import fmt "fmt"
import math "math"
import wrappers "github.com/golang/protobuf/ptypes/wrappers"
import protobuf "github.com/netbrain/tf-grpc/go/tensorflow/tensorflow/go/core/protobuf"

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package

// Configuration parameters for a SessionBundle, with optional batching.
type SessionBundleConfig struct {
	// The TensorFlow runtime to connect to.
	// See full documentation in tensorflow/core/public/session_options.h.
	//
	// For single machine serving, we recommend using the empty string "", which
	// will configure the local TensorFlow runtime implementation. This provides
	// the best isolation currently available across multiple Session servables.
	SessionTarget string `protobuf:"bytes,1,opt,name=session_target,json=sessionTarget,proto3" json:"session_target,omitempty"`
	// TensorFlow Session configuration options.
	// See details at tensorflow/core/protobuf/config.proto.
	SessionConfig *protobuf.ConfigProto `protobuf:"bytes,2,opt,name=session_config,json=sessionConfig,proto3" json:"session_config,omitempty"`
	// If set, each emitted session is wrapped with a layer that schedules Run()
	// calls in batches. The batching layer is transparent to the client
	// (implements the tensorflow::Session API).
	//
	// IMPORTANT: With batching enabled, client threads will spend most of their
	// time blocked on Session::Run() calls, waiting for enough peer threads to
	// also call Session::Run() such that a large batch can be formed. For good
	// throughput, we recommend setting the number of client threads equal to
	// roughly twice the maximum batch size ('max_batch_size' below).
	//
	// The batching layer uses a SharedBatchScheduler to coordinate batching
	// across multiple session servables emitted by this source adapter. A
	// BatchSchedulerRetrier is added on top of each batching session.
	BatchingParameters *BatchingParameters `protobuf:"bytes,3,opt,name=batching_parameters,json=batchingParameters,proto3" json:"batching_parameters,omitempty"`
	// If set, session run calls use a separate threadpool for restore and init
	// ops as part of loading the session-bundle. The value of this field should
	// correspond to the index of the tensorflow::ThreadPoolOptionProto defined as
	// part of `session_config.session_inter_op_thread_pool`.
	SessionRunLoadThreadpoolIndex *wrappers.Int32Value `protobuf:"bytes,4,opt,name=session_run_load_threadpool_index,json=sessionRunLoadThreadpoolIndex,proto3" json:"session_run_load_threadpool_index,omitempty"`
	// EXPERIMENTAL. THIS FIELD MAY CHANGE OR GO AWAY. USE WITH CAUTION.
	//
	// Transient memory used while loading a model, which is released once the
	// loading phase has completed. (This is on top of the memory used in steady-
	// state while the model is in memory after it has finished loading.)
	//
	// TODO(b/38376838): This is a temporary hack, and it applies to all models.
	// Remove it once resource estimates are moved inside SavedModel.
	ExperimentalTransientRamBytesDuringLoad uint64 `protobuf:"varint,5,opt,name=experimental_transient_ram_bytes_during_load,json=experimentalTransientRamBytesDuringLoad,proto3" json:"experimental_transient_ram_bytes_during_load,omitempty"`
	// Set of SavedModel tags identifying the specific meta graph def to be
	// loaded.
	SavedModelTags []string `protobuf:"bytes,6,rep,name=saved_model_tags,json=savedModelTags,proto3" json:"saved_model_tags,omitempty"`
	// EXPERIMENTAL. THIS FIELD MAY CHANGE OR GO AWAY. USE WITH CAUTION.
	//
	// Input tensors to append to every Session::Run() call.
	ExperimentalFixedInputTensors []*protobuf.NamedTensorProto `protobuf:"bytes,778,rep,name=experimental_fixed_input_tensors,json=experimentalFixedInputTensors,proto3" json:"experimental_fixed_input_tensors,omitempty"`
	// Enables model warmup.
	EnableModelWarmup    bool     `protobuf:"varint,779,opt,name=enable_model_warmup,json=enableModelWarmup,proto3" json:"enable_model_warmup,omitempty"`
	XXX_NoUnkeyedLiteral struct{} `json:"-"`
	XXX_unrecognized     []byte   `json:"-"`
	XXX_sizecache        int32    `json:"-"`
}

func (m *SessionBundleConfig) Reset()         { *m = SessionBundleConfig{} }
func (m *SessionBundleConfig) String() string { return proto.CompactTextString(m) }
func (*SessionBundleConfig) ProtoMessage()    {}
func (*SessionBundleConfig) Descriptor() ([]byte, []int) {
	return fileDescriptor_session_bundle_config_63635253a6d25a93, []int{0}
}
func (m *SessionBundleConfig) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_SessionBundleConfig.Unmarshal(m, b)
}
func (m *SessionBundleConfig) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_SessionBundleConfig.Marshal(b, m, deterministic)
}
func (dst *SessionBundleConfig) XXX_Merge(src proto.Message) {
	xxx_messageInfo_SessionBundleConfig.Merge(dst, src)
}
func (m *SessionBundleConfig) XXX_Size() int {
	return xxx_messageInfo_SessionBundleConfig.Size(m)
}
func (m *SessionBundleConfig) XXX_DiscardUnknown() {
	xxx_messageInfo_SessionBundleConfig.DiscardUnknown(m)
}

var xxx_messageInfo_SessionBundleConfig proto.InternalMessageInfo

func (m *SessionBundleConfig) GetSessionTarget() string {
	if m != nil {
		return m.SessionTarget
	}
	return ""
}

func (m *SessionBundleConfig) GetSessionConfig() *protobuf.ConfigProto {
	if m != nil {
		return m.SessionConfig
	}
	return nil
}

func (m *SessionBundleConfig) GetBatchingParameters() *BatchingParameters {
	if m != nil {
		return m.BatchingParameters
	}
	return nil
}

func (m *SessionBundleConfig) GetSessionRunLoadThreadpoolIndex() *wrappers.Int32Value {
	if m != nil {
		return m.SessionRunLoadThreadpoolIndex
	}
	return nil
}

func (m *SessionBundleConfig) GetExperimentalTransientRamBytesDuringLoad() uint64 {
	if m != nil {
		return m.ExperimentalTransientRamBytesDuringLoad
	}
	return 0
}

func (m *SessionBundleConfig) GetSavedModelTags() []string {
	if m != nil {
		return m.SavedModelTags
	}
	return nil
}

func (m *SessionBundleConfig) GetExperimentalFixedInputTensors() []*protobuf.NamedTensorProto {
	if m != nil {
		return m.ExperimentalFixedInputTensors
	}
	return nil
}

func (m *SessionBundleConfig) GetEnableModelWarmup() bool {
	if m != nil {
		return m.EnableModelWarmup
	}
	return false
}

// Batching parameters. Each individual parameter is optional. If omitted, the
// default value from the relevant batching config struct (SharedBatchScheduler
// ::Options or BatchSchedulerRetrier::Options) is used.
type BatchingParameters struct {
	// The maximum size of each batch.
	//
	// IMPORTANT: As discussed above, use 'max_batch_size * 2' client threads to
	// achieve high throughput with batching.
	MaxBatchSize *wrappers.Int64Value `protobuf:"bytes,1,opt,name=max_batch_size,json=maxBatchSize,proto3" json:"max_batch_size,omitempty"`
	// If a task has been enqueued for this amount of time (in microseconds), and
	// a thread is available, the scheduler will immediately form a batch from
	// enqueued tasks and assign the batch to the thread for processing, even if
	// the batch's size is below 'max_batch_size'.
	BatchTimeoutMicros *wrappers.Int64Value `protobuf:"bytes,2,opt,name=batch_timeout_micros,json=batchTimeoutMicros,proto3" json:"batch_timeout_micros,omitempty"`
	// The maximum length of the queue, in terms of the number of batches. (A
	// batch that has been scheduled on a thread is considered to have been
	// removed from the queue.)
	MaxEnqueuedBatches *wrappers.Int64Value `protobuf:"bytes,3,opt,name=max_enqueued_batches,json=maxEnqueuedBatches,proto3" json:"max_enqueued_batches,omitempty"`
	// The number of threads to use to process batches.
	// Must be >= 1, and should be tuned carefully.
	NumBatchThreads *wrappers.Int64Value `protobuf:"bytes,4,opt,name=num_batch_threads,json=numBatchThreads,proto3" json:"num_batch_threads,omitempty"`
	// The name to use for the pool of batch threads.
	ThreadPoolName *wrappers.StringValue `protobuf:"bytes,5,opt,name=thread_pool_name,json=threadPoolName,proto3" json:"thread_pool_name,omitempty"`
	// The allowed batch sizes. (Ignored if left empty.)
	// Requirements:
	//  - The entries must be in increasing order.
	//  - The final entry must equal 'max_batch_size'.
	AllowedBatchSizes []int64 `protobuf:"varint,6,rep,packed,name=allowed_batch_sizes,json=allowedBatchSizes,proto3" json:"allowed_batch_sizes,omitempty"`
	// Whether to pad variable-length inputs when a batch is formed.
	PadVariableLengthInputs bool     `protobuf:"varint,7,opt,name=pad_variable_length_inputs,json=padVariableLengthInputs,proto3" json:"pad_variable_length_inputs,omitempty"`
	XXX_NoUnkeyedLiteral    struct{} `json:"-"`
	XXX_unrecognized        []byte   `json:"-"`
	XXX_sizecache           int32    `json:"-"`
}

func (m *BatchingParameters) Reset()         { *m = BatchingParameters{} }
func (m *BatchingParameters) String() string { return proto.CompactTextString(m) }
func (*BatchingParameters) ProtoMessage()    {}
func (*BatchingParameters) Descriptor() ([]byte, []int) {
	return fileDescriptor_session_bundle_config_63635253a6d25a93, []int{1}
}
func (m *BatchingParameters) XXX_Unmarshal(b []byte) error {
	return xxx_messageInfo_BatchingParameters.Unmarshal(m, b)
}
func (m *BatchingParameters) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {
	return xxx_messageInfo_BatchingParameters.Marshal(b, m, deterministic)
}
func (dst *BatchingParameters) XXX_Merge(src proto.Message) {
	xxx_messageInfo_BatchingParameters.Merge(dst, src)
}
func (m *BatchingParameters) XXX_Size() int {
	return xxx_messageInfo_BatchingParameters.Size(m)
}
func (m *BatchingParameters) XXX_DiscardUnknown() {
	xxx_messageInfo_BatchingParameters.DiscardUnknown(m)
}

var xxx_messageInfo_BatchingParameters proto.InternalMessageInfo

func (m *BatchingParameters) GetMaxBatchSize() *wrappers.Int64Value {
	if m != nil {
		return m.MaxBatchSize
	}
	return nil
}

func (m *BatchingParameters) GetBatchTimeoutMicros() *wrappers.Int64Value {
	if m != nil {
		return m.BatchTimeoutMicros
	}
	return nil
}

func (m *BatchingParameters) GetMaxEnqueuedBatches() *wrappers.Int64Value {
	if m != nil {
		return m.MaxEnqueuedBatches
	}
	return nil
}

func (m *BatchingParameters) GetNumBatchThreads() *wrappers.Int64Value {
	if m != nil {
		return m.NumBatchThreads
	}
	return nil
}

func (m *BatchingParameters) GetThreadPoolName() *wrappers.StringValue {
	if m != nil {
		return m.ThreadPoolName
	}
	return nil
}

func (m *BatchingParameters) GetAllowedBatchSizes() []int64 {
	if m != nil {
		return m.AllowedBatchSizes
	}
	return nil
}

func (m *BatchingParameters) GetPadVariableLengthInputs() bool {
	if m != nil {
		return m.PadVariableLengthInputs
	}
	return false
}

func init() {
	proto.RegisterType((*SessionBundleConfig)(nil), "tensorflow.serving.SessionBundleConfig")
	proto.RegisterType((*BatchingParameters)(nil), "tensorflow.serving.BatchingParameters")
}

func init() {
	proto.RegisterFile("tensorflow_serving/servables/tensorflow/session_bundle_config.proto", fileDescriptor_session_bundle_config_63635253a6d25a93)
}

var fileDescriptor_session_bundle_config_63635253a6d25a93 = []byte{
	// 696 bytes of a gzipped FileDescriptorProto
	0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0x7c, 0x54, 0xe1, 0x6a, 0x13, 0x4b,
	0x18, 0x25, 0x37, 0xbd, 0xbd, 0xb7, 0xd3, 0x7b, 0x63, 0x3b, 0x11, 0xba, 0x44, 0x2b, 0xb1, 0x50,
	0x0d, 0xa8, 0xbb, 0xd0, 0x8a, 0x20, 0x05, 0xc1, 0x54, 0x2b, 0x85, 0x56, 0xca, 0x36, 0xb4, 0x20,
	0xc8, 0x30, 0x9b, 0xfd, 0xb2, 0x19, 0xd8, 0x99, 0x59, 0x67, 0x66, 0x9b, 0xd8, 0x47, 0xd0, 0x77,
	0xf4, 0x25, 0x7c, 0x01, 0x99, 0x99, 0xdd, 0x74, 0x35, 0xb4, 0xbf, 0x4a, 0xe7, 0x3b, 0xe7, 0xcc,
	0x99, 0xef, 0x9c, 0x2c, 0x3a, 0x34, 0x20, 0xb4, 0x54, 0x93, 0x5c, 0xce, 0x88, 0x06, 0x75, 0xc5,
	0x44, 0x16, 0xd9, 0xbf, 0x34, 0xc9, 0x41, 0x47, 0x37, 0xc3, 0x48, 0x83, 0xd6, 0x4c, 0x0a, 0x92,
	0x94, 0x22, 0xcd, 0x81, 0x8c, 0xa5, 0x98, 0xb0, 0x2c, 0x2c, 0x94, 0x34, 0x12, 0xe3, 0x1b, 0x5c,
	0x58, 0x89, 0xf4, 0x1e, 0x65, 0x52, 0x66, 0x39, 0x44, 0x0e, 0x91, 0x94, 0x93, 0x68, 0xa6, 0x68,
	0x51, 0x80, 0xd2, 0x9e, 0xd3, 0xdb, 0x6d, 0x68, 0x8f, 0xa5, 0x6a, 0x00, 0x9b, 0xd2, 0xbd, 0x67,
	0xb7, 0xc2, 0x04, 0xe5, 0x90, 0x12, 0x3f, 0xf6, 0xe0, 0x9d, 0x1f, 0x2b, 0xa8, 0x7b, 0xee, 0x7d,
	0x0e, 0x9d, 0xcd, 0x43, 0x27, 0x85, 0x77, 0x51, 0xa7, 0xb6, 0x6f, 0xa8, 0xca, 0xc0, 0x04, 0xad,
	0x7e, 0x6b, 0xb0, 0x16, 0xff, 0x5f, 0x9d, 0x8e, 0xdc, 0x21, 0x7e, 0x73, 0x03, 0xf3, 0x1e, 0x82,
	0xbf, 0xfa, 0xad, 0xc1, 0xfa, 0xde, 0x56, 0xd8, 0x78, 0x9f, 0x97, 0x3c, 0xb3, 0xf7, 0x2d, 0xf8,
	0xd5, 0x35, 0x97, 0xa8, 0x9b, 0x50, 0x33, 0x9e, 0x32, 0x91, 0x91, 0x82, 0x2a, 0xca, 0xc1, 0x80,
	0xd2, 0x41, 0xdb, 0x89, 0x3c, 0x09, 0x97, 0x97, 0x14, 0x0e, 0x2b, 0xf8, 0xd9, 0x02, 0x1d, 0xe3,
	0x64, 0xe9, 0x0c, 0x03, 0x7a, 0x5c, 0x1b, 0x53, 0xa5, 0x20, 0xb9, 0xa4, 0x29, 0x31, 0x53, 0x05,
	0x34, 0x2d, 0xa4, 0xcc, 0x09, 0x13, 0x29, 0xcc, 0x83, 0x15, 0x77, 0xcd, 0x83, 0xd0, 0xef, 0x3d,
	0xac, 0xf7, 0x14, 0x1e, 0x0b, 0xb3, 0xbf, 0x77, 0x41, 0xf3, 0x12, 0xe2, 0xed, 0x4a, 0x25, 0x2e,
	0xc5, 0x89, 0xa4, 0xe9, 0x68, 0x21, 0x71, 0x6c, 0x15, 0xf0, 0x67, 0xf4, 0x1c, 0xe6, 0x05, 0x28,
	0xc6, 0x41, 0x18, 0x9a, 0x13, 0xa3, 0xa8, 0xd0, 0x0c, 0x84, 0x21, 0x8a, 0x72, 0x92, 0x7c, 0x35,
	0xa0, 0x49, 0x5a, 0x2a, 0xfb, 0x3c, 0x6b, 0x21, 0xf8, 0xbb, 0xdf, 0x1a, 0xac, 0xc4, 0x4f, 0x9b,
	0x9c, 0x51, 0x4d, 0x89, 0x29, 0x1f, 0x5a, 0xc2, 0x3b, 0x87, 0xb7, 0xb7, 0xe1, 0x01, 0xda, 0xd0,
	0xf4, 0x0a, 0x52, 0xc2, 0x65, 0x0a, 0x39, 0x31, 0x34, 0xd3, 0xc1, 0x6a, 0xbf, 0x3d, 0x58, 0x8b,
	0x3b, 0xee, 0xfc, 0xd4, 0x1e, 0x8f, 0x68, 0xa6, 0xf1, 0x04, 0xf5, 0x7f, 0x33, 0x32, 0x61, 0x73,
	0x48, 0x09, 0x13, 0x45, 0x69, 0xaa, 0xc0, 0x75, 0xf0, 0xcd, 0x52, 0xd7, 0xf7, 0x1e, 0x36, 0xd7,
	0xfa, 0xd1, 0x56, 0x62, 0xe4, 0xfe, 0xf7, 0x01, 0x6d, 0x37, 0x65, 0x8e, 0xac, 0xca, 0xb1, 0x15,
	0xf1, 0x10, 0x8d, 0x23, 0xd4, 0x05, 0x61, 0x9b, 0x5e, 0x59, 0x9a, 0x51, 0xc5, 0xcb, 0x22, 0xf8,
	0xbe, 0xda, 0x6f, 0x0d, 0xfe, 0x8d, 0x37, 0xfd, 0xcc, 0xd9, 0xba, 0x74, 0x93, 0x9d, 0x9f, 0x6d,
	0x84, 0x97, 0x33, 0xc3, 0x6f, 0x51, 0x87, 0xd3, 0x39, 0x71, 0xc9, 0x11, 0xcd, 0xae, 0xc1, 0xf5,
	0xeb, 0x96, 0x30, 0x5e, 0xbd, 0xf4, 0x61, 0xfc, 0xc7, 0xe9, 0xdc, 0x69, 0x9d, 0xb3, 0x6b, 0xc0,
	0xa7, 0xe8, 0xbe, 0xa7, 0x1b, 0xc6, 0x41, 0x96, 0x86, 0x70, 0x36, 0x56, 0x52, 0x57, 0x0d, 0xbc,
	0x53, 0xc8, 0x37, 0x66, 0xe4, 0x79, 0xa7, 0x8e, 0x66, 0xe5, 0xac, 0x23, 0x10, 0x5f, 0x4a, 0x28,
	0x21, 0xf5, 0xd6, 0xa0, 0xee, 0xe2, 0xdd, 0x72, 0x9c, 0xce, 0xdf, 0x57, 0xbc, 0xa1, 0xa7, 0xe1,
	0x0f, 0x68, 0x53, 0x94, 0xbc, 0x7a, 0xa0, 0x6f, 0x9e, 0xbe, 0xab, 0x70, 0xb5, 0xd6, 0x3d, 0x51,
	0x72, 0xa7, 0xe1, 0xab, 0xa6, 0xf1, 0x11, 0xda, 0xf0, 0x74, 0xe2, 0x9a, 0x6b, 0x7f, 0xc3, 0xae,
	0x46, 0x36, 0xc8, 0x3f, 0x75, 0xce, 0x8d, 0xad, 0x8e, 0x17, 0xea, 0x78, 0xd6, 0x99, 0x94, 0xb9,
	0x0d, 0x19, 0x87, 0xa8, 0x4b, 0xf3, 0x5c, 0xce, 0xea, 0xa7, 0xb9, 0xad, 0xfb, 0x3a, 0xb5, 0xe3,
	0xcd, 0x6a, 0xb4, 0xd8, 0xae, 0xc6, 0x07, 0xa8, 0x57, 0xd0, 0x94, 0x5c, 0x51, 0xc5, 0x5c, 0xde,
	0x39, 0x88, 0xcc, 0x4c, 0x7d, 0xa5, 0x74, 0xf0, 0x8f, 0xcb, 0x7b, 0xab, 0xa0, 0xe9, 0x45, 0x05,
	0x38, 0x71, 0x73, 0x57, 0x16, 0x3d, 0x3c, 0xf8, 0xf4, 0x3a, 0x63, 0x66, 0x5a, 0x26, 0xe1, 0x58,
	0xf2, 0x48, 0x80, 0x49, 0x14, 0x65, 0x22, 0x32, 0x93, 0x17, 0x99, 0x2a, 0xc6, 0x51, 0x26, 0xa3,
	0xfa, 0xcb, 0xb9, 0xfc, 0x31, 0x4d, 0x56, 0xdd, 0x7b, 0xf6, 0x7f, 0x05, 0x00, 0x00, 0xff, 0xff,
	0x82, 0x7f, 0x14, 0xe3, 0x69, 0x05, 0x00, 0x00,
}
